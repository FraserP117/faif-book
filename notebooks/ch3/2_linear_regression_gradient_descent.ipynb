{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing linear regression with maximum likelihood estimation\n",
    "\n",
    "Using the agent to infer linear generating function parameters instead of hidden states via gradient descent instead of analytic calculation of the maximum likelihood estimate.\n",
    "\n",
    "==========================================================================\n",
    "\n",
    "* **Notebook dependencies**:\n",
    "    * ...\n",
    "\n",
    "* **Content**: Jupyter notebook accompanying Chapter 3 of the textbook \"Fundamentals of Active Inference\"\n",
    "\n",
    "* **Author**: Sanjeev Namjoshi (sanjeev.namjoshi@gmail.com)\n",
    "\n",
    "* **Version**: 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "from matplotlib import cm\n",
    "from torch.distributions import Normal\n",
    "from typing import Union\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(os.pardir, os.pardir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from src.utils import create_environment\n",
    "\n",
    "mpl.style.use(\"seaborn-deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook recapitulates the ideas from the previous notebooks but uses gradient descent instead of the analytic update rules for maximum likelihood estimation. In Chapter 2, we used gradient descent for hidden state estimation. This notebook will be instead be estimating the parameters of the linear model, assuming that $x$ is known. We will use the following objective function, the negative log likelihood\n",
    "\n",
    "$$\n",
    "    -\\ell(\\theta) = - \\sum_{i=1}^N \\log \\mathcal{N}(y^{(i)}; \\beta_0 + \\beta_1 x^{(i)}, \\sigma^2_y)\n",
    "$$\n",
    "\n",
    "where $\\theta \\in \\left \\{\\beta_0, \\beta_1 \\right \\}$. We can obtain the log-likelihood function over samples by modifying the MLE objective defined in Notebook 2.9. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generating_function(beta_0: float, beta_1: float, x: float) -> float:\n",
    "    return beta_1 * x + beta_0\n",
    "\n",
    "def mle_objective(x: float, y: Union[float, torch.tensor], theta: torch.tensor) -> torch.tensor:\n",
    "    \n",
    "    # Parameters\n",
    "    beta_0 = theta[0]  # Linear generating function intercept\n",
    "    beta_1 = theta[1]  # Linear generating function slope\n",
    "    var_y  = 0.5       # Likelihood variance\n",
    "    \n",
    "    # Linear genearting function\n",
    "    mu_y   = generating_function(beta_0=beta_0, beta_1=beta_1, x=x)\n",
    "    \n",
    "    # Calculate log-likelihood over samples    \n",
    "    log_likelihood = Normal(loc=mu_y, scale=np.sqrt(var_y)).log_prob(y).sum(axis=0)\n",
    "    \n",
    "    return -log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In gradient descent we update our parameter in the direction of the negative gradient along our loss function. This means that in order to perform gradient descent we need to pick a starting position on the loss function (the initialization), a learning rate specifying how big we are going to step down the loss function, and then loop through the update process over $j$ iterations. The gradient descent process is captured by the following equation:\n",
    "\n",
    "$$\n",
    "\\theta^{(j+1)} \\gets \\theta^{(j)} - \\kappa \\frac{\\partial \\ell(\\theta)}{\\partial \\theta^{(j)}}\n",
    "$$\n",
    "\n",
    "In the code below, we initialize the parameters randomly from -100 to 100. The initial loss will be calculated at this random initialization for the parameters. Note that the parameters include two values: $\\beta_0$ and $\\beta_1$. `pytorch` will automatically determine the gradient update for both of these parameters simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_0_init = np.random.uniform(low=-100, high=100)\n",
    "beta_1_init = np.random.uniform(low=-100, high=100)\n",
    "\n",
    "theta = np.array([beta_0_init, beta_1_init])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we specify the code for gradient descent, also modified from Notebook 2.9. The only change here is to specify $\\theta$ as an input to the function and allowing the objective function to take $\\theta$, $x$, and $y$ as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(kappa: float, \n",
    "                     n_iterations: int, \n",
    "                     theta: np.array, \n",
    "                     obj: callable):\n",
    "    \n",
    "    print(f\"Initializing theta at {theta}.\")\n",
    "    \n",
    "    # Initialize empty history arrays\n",
    "    theta_history = torch.zeros((n_iterations, 2))\n",
    "    loss_history  = torch.zeros((n_iterations, 2))\n",
    "    \n",
    "    # Turn x into a Torch tensor which is differentiable\n",
    "    theta = torch.tensor(theta, requires_grad=True)\n",
    "    \n",
    "    # Calculate loss at initialization\n",
    "    loss = obj(x_star, y, theta)\n",
    "    \n",
    "    # Add initialization values to history (j=0)\n",
    "    theta_history[0] = theta\n",
    "    loss_history[0]  = loss\n",
    "    \n",
    "    # Gradient descent algorithm (for j+1...n_iterations)\n",
    "    for j in range(n_iterations-1):\n",
    "        obj_theta = obj(x_star, y, theta)  # Compute loss\n",
    "        obj_theta.backward()   # Compute gradient of tensor\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            theta -= (kappa * theta.grad)   # Step in direction of gradient\n",
    "            theta.grad.zero_()              # Zero out the gradients\n",
    "        \n",
    "        # Recalculate loss\n",
    "        loss = obj(x_star, y, theta)\n",
    "        \n",
    "        # Append to history\n",
    "        theta_history[j+1] = theta\n",
    "        loss_history[j+1]  = loss\n",
    "\n",
    "    print(f\"Final value of theta: {np.round(theta.detach().numpy(), 3)}\")\n",
    "    history = {\"theta\": theta_history, \"loss\": loss_history}\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create the environment and generate data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment parameters\n",
    "env_params = {\n",
    "    \"beta_0_star\" : 3,    # Linear parameter intercept\n",
    "    \"beta_1_star\" : 2,    # Linear parameter slope\n",
    "    \"y_star_std\"  : 1     # Standard deviation of sensory data\n",
    "}\n",
    "\n",
    "env = create_environment(name=\"static_linear\", params=env_params)\n",
    "\n",
    "# Generate data from the support of x\n",
    "N       = 500                                        # Number of samples\n",
    "x_range = np.linspace(start=0.01, stop=5, num=500)   # Support of x\n",
    "x_star  = np.random.choice(x_range, size=N)          # N random external states\n",
    "y       = np.zeros(N)                                # Empty array for N data samples\n",
    "\n",
    "# Generate N samples\n",
    "for idx, x in enumerate(x_star):\n",
    "    env.build(x)\n",
    "    y[idx] = env.generate()\n",
    "\n",
    "# Convert numpy array for data into tensor\n",
    "y      = torch.tensor(y)\n",
    "x_star = torch.tensor(x_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And run gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing theta at [ 54.08689628 -57.9462361 ].\n",
      "Final value of theta: [3.107 1.96 ]\n"
     ]
    }
   ],
   "source": [
    "mle_history = gradient_descent(\n",
    "    kappa=0.0001, \n",
    "    n_iterations=700, \n",
    "    theta=theta, \n",
    "    obj=mle_objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_history = mle_history[\"theta\"]\n",
    "loss_history = mle_history[\"loss\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, gradient descent was successful. To visualize the results we plot the iterative updates to the values of $\\beta_0$ and $\\beta_1$ on the surface of the loss function in 2D and 3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_43886/2751481286.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Vectorize NLL and compute the costs at all combinations of beta_0 and beta_1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mnll_mle_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmle_objective\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnll_mle_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Plot the contours of the costs and fill them in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ds_env_1/lib/python3.9/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2302\u001b[0m             \u001b[0mvargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_n\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_n\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2304\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vectorize_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2306\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_ufunc_and_otypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ds_env_1/lib/python3.9/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_vectorize_call\u001b[0;34m(self, func, args)\u001b[0m\n\u001b[1;32m   2380\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2381\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2382\u001b[0;31m             \u001b[0mufunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0motypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_ufunc_and_otypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2384\u001b[0m             \u001b[0;31m# Convert args to object arrays first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ds_env_1/lib/python3.9/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_get_ufunc_and_otypes\u001b[0;34m(self, func, args)\u001b[0m\n\u001b[1;32m   2334\u001b[0m             \u001b[0;31m# Assumes that ufunc first evaluates the 0th elements in the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2335\u001b[0m             \u001b[0;31m# arrays (the input values are not checked to ensure this)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2336\u001b[0;31m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2337\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2338\u001b[0m                 raise ValueError('cannot call `vectorize` on size 0 inputs '\n",
      "\u001b[0;32m~/anaconda3/envs/ds_env_1/lib/python3.9/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2334\u001b[0m             \u001b[0;31m# Assumes that ufunc first evaluates the 0th elements in the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2335\u001b[0m             \u001b[0;31m# arrays (the input values are not checked to ensure this)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2336\u001b[0;31m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2337\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2338\u001b[0m                 raise ValueError('cannot call `vectorize` on size 0 inputs '\n",
      "\u001b[0;32m~/anaconda3/envs/ds_env_1/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    971\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1008x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "fig = plt.figure(facecolor=(1,1,1), figsize=(14,5))\n",
    "\n",
    "\"\"\" LEFT PLOT \"\"\"\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "\n",
    "# Set up grid\n",
    "b0_range = np.linspace(-100, 100, 200)\n",
    "b1_range = np.linspace(-100, 100, 200)\n",
    "X, Y = np.meshgrid(b0_range, b1_range)\n",
    "\n",
    "# Vectorize NLL and compute the costs at all combinations of beta_0 and beta_1\n",
    "nll_mle_vec = np.vectorize(mle_objective)\n",
    "z = nll_mle_vec(X, Y, theta_history)\n",
    "\n",
    "# Plot the contours of the costs and fill them in\n",
    "contours = ax.contour(X, Y, z, levels=15, colors='k')\n",
    "fill = ax.contourf(X, Y, z, levels=50, cmap=\"viridis\")\n",
    "\n",
    "# Compute the updates of beta_0 and beta_1 for each iteration of gradient descent\n",
    "beta_0 = [theta[0] for theta in theta_history]\n",
    "beta_1 = [theta[1] for theta in theta_history]\n",
    "\n",
    "# Plot the beta_0 and beta_1 updates across the loss function\n",
    "ax.plot(beta_0, beta_1, \"r>\", ms=2.5)\n",
    "ax.plot(3, 2, marker=\"+\", color=\"black\", markersize=12)   # True generating parameters\n",
    "\n",
    "# Axis labels\n",
    "ax.set_xlabel(\"beta_0\", fontsize=18)\n",
    "ax.set_ylabel(\"beta_1\", fontsize=18)\n",
    "\n",
    "# Labels contours and add colorbar\n",
    "plt.clabel(contours)\n",
    "plt.colorbar(fill)\n",
    "\n",
    "\"\"\" RIGHT PLOT \"\"\"\n",
    "ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "\n",
    "# Plot 3D negative log-likelihood\n",
    "ax.plot_surface(X, Y, z, cmap=cm.viridis, antialiased=False, alpha=0.75)\n",
    "\n",
    "# Compute the costs at each weight update\n",
    "loss_at_theta = [mle_objective(theta[0], theta[1]).tolist() for theta in theta_history]\n",
    "\n",
    "# Plot update along loss surface \n",
    "ax.plot(beta_0, beta_1, loss_at_theta, \"r>\", ms=1.5, zorder=10)\n",
    "\n",
    "# Change 3D plot camera view\n",
    "ax.view_init(20, 180)\n",
    "\n",
    "# Axis labels\n",
    "ax.set_xlabel(\"beta_0\", fontsize=18)\n",
    "ax.set_ylabel(\"beta_1\", fontsize=18)\n",
    "ax.set_zlabel(\"loss\", fontsize=18)\n",
    "\n",
    "ax.dist = 7.5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
