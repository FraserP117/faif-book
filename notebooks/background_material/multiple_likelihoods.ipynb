{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to think about likelihoods over multiple samples\n",
    "\n",
    "This notebook demonstrates how we treate a likelihood over multiple samples.\n",
    "\n",
    "==========================================================================\n",
    "\n",
    "* **Notebook dependencies**:\n",
    "    * ...\n",
    "\n",
    "* **Content**: Jupyter notebook accompanying Chapter 2 of the textbook \"Fundamentals of Active Inference\"\n",
    "\n",
    "* **Author**: Sanjeev Namjoshi (sanjeev.namjoshi@gmail.com)\n",
    "\n",
    "* **Version**: 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a previous notebook we showed how to scale hidden state inference with Bayes' theorem to multiple samples. In this notebook we will extend the explanation of the likelihood a step further. \n",
    "\n",
    "We will use the same assume the following:\n",
    "* $x^*$: The true **external state** of the generative process.\n",
    "* $y$: The **outcome** of a generative process, known as the **observation** for a generative model. This is the data the agent receives.\n",
    "\n",
    "In this scenario the external states of the generative process ($x^*$) denote the size of a food source and the outcomes ($y$) are represents levels of light intensity emitted from the food as a function of size. Using these observations, the agent needs to infer (\"perceive\") the hidden state of the generative process that generated the data it is receiving. This is represented by the variable ($x$), the **hidden state** which captures the agent's belief about the food size that could have generated the observed sensory data. We use the following agent and environment:\n",
    "\n",
    "$$\n",
    "    \\mathscr{E} \\triangleq \n",
    "    \\begin{cases}\n",
    "        y = g_{\\mathscr{E}}(x^*; \\theta^*) + \\omega_y^*    & \\text{Outcome generation} \\\\\n",
    "        g_{\\mathscr{E}}(x^*; \\theta^*) = \\beta_0^* + \\beta_1^* x^* & \\text{Generating function} \\\\\n",
    "        \\omega_y^* \\sim \\mathcal{N}(0, \\sigma^2=1) & \\text{Observation noise} \\\\\n",
    "        \\theta^* := \\left \\{\\beta_0^* = 3, \\beta_1^* = 2 \\right \\} & \\text{Observation parameters}\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\mathcal{M} \\triangleq \n",
    "    \\begin{cases}\n",
    "        p_{\\mu_y, \\sigma^2_y}(y_i \\mid x) = \\mathcal{N}(y_i; g_{\\mathcal{M}}, \\sigma^2_y) & \\text{Likelihood} \\\\\n",
    "        p_{\\mu_y, \\sigma^2_y}(x) = \\mathcal{N}(x; m_x, s^2_x) & \\text{Prior on } x \\\\\n",
    "        g_{\\mathcal{M}}(x, \\theta) = \\beta_0 + \\beta_1 x & \\text{Generating function} \\\\\n",
    "        \\theta := \\left \\{\\beta_0 = 3, \\beta_1 = 2 \\right \\}  & \\text{Linear parameters} \\\\ \n",
    "        \\phi := \\left \\{\\sigma^2_y = 0.25, s^2_x = 0.25, m_x = 4 \\right \\} & \\text{Other parameters}\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "The only difference is that observation are now indexed by $i = 0, \\dots, N$ to indicate multiple samples are generated from the same hidden state. We use the linear environment we have used previously and generate $N=30$ samples with $x^*=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
