{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytic solutions to most likely hidden state by MLE/MAP estimation\n",
    "\n",
    "This notebook shows how one can use maximum likelihood estimation (MLE) or maximum a posteriori (MAP) estimation to find the most likely hidden state given some sensory observation.\n",
    "\n",
    "==========================================================================\n",
    "\n",
    "* **Notebook dependencies**:\n",
    "    * ...\n",
    "\n",
    "* **Content**: Jupyter notebook accompanying Chapter 2 of the textbook \"Fundamentals of Active Inference\"\n",
    "\n",
    "* **Author**: Sanjeev Namjoshi (sanjeev.namjoshi@gmail.com)\n",
    "\n",
    "* **Version**: 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from types import SimpleNamespace\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(os.pardir, os.pardir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from src.utils import create_environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Chapter 2 we introduced a strategy for finding the most likely value of the hidden state given sensory data. This approach is non-Bayesian because it does not directly involve the calculation of Bayes' theorem. Rather, this approach exploits the algebraic manipulation of the Gaussian distributions for the likelihood (and/or prior) to determine an analytic update for the hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum likelihood of the hidden state\n",
    "\n",
    "We will first examine the **maximum likelihood estimate** (MLE) of the hidden state. To obtain this quantity we first start with the likelihood function in our model.\n",
    "\n",
    "$$\n",
    "    \\mathcal{L}(x) \\triangleq \\prod_{i=0}^{N} p(y^{(i)} \\mid x)\n",
    "$$\n",
    "\n",
    "Here we have emphasized that the likelihood is a function of $x$. \n",
    "\n",
    "\n",
    "To obtain the MLE, what we want to know is: \"Which value of $x$ is has the highest credibility for having produced the observation $y^{(i)}$?\" We write this as\n",
    "\n",
    "$$\n",
    "    x^{MLE} \\triangleq \\underset{x}{\\text{argmax}} \\hspace{1mm} \\mathcal{L}(x)\n",
    "$$\n",
    "\n",
    "This represents our **loss function** or **objective** for finding the optimal hidden state estimate. It is customary to take the log of this quantity to turn the product into a sum and obtain the log-likelihood for the MLE. We denote this with the $\\ell$ symbol as follows:\n",
    "\n",
    "$$\n",
    "    \\ell(x) \\triangleq \\log \\mathcal{L}(x) = \\sum_{i=0}^N \\log p(y^{(i)} \\mid x) \n",
    "$$\n",
    "\n",
    "Finally, we will also want to take the negative of this quantity, as is tradition, so we minimize this loss function instead of maximizing it. The objective thus becomes\n",
    "\n",
    "$$\n",
    "    x^{MLE} \\triangleq \\underset{x}{\\text{argmin}} \\hspace{1mm} -\\sum^N_{i=0} \\log p(y^{(i)} \\mid x) = -\\ell(x)\n",
    "$$\n",
    "\n",
    "First, we will assume the likelihood is Gaussian with a linear generating function. Then we can use the equation for the normal distribution, substitute in the linear generating function for the mean of this distribution in the equation, and take the negative log. This produces an algebraic equation for the negative log likelihood, our loss function. Finally, we take the partial derivative of this loss function with respect to the unknown variable, in this case the hidden state $x$, set it equal to zero, and solve for it. In this way, we solve for the value that minimizes the objective which is the following:\n",
    "\n",
    "$$\n",
    "    x^{MLE} = \\frac{\\bar{y} - \\beta_0}{\\beta_1}\n",
    "$$\n",
    "\n",
    "where $\\bar{y}$ denotes the average over samples of $y$. \n",
    "\n",
    "Note that in doing so we do not get the entire posterior distribution but instead the **posterior mode** (which in the case of the Gaussian is identical to the mean). This is why the method is non-Bayesian. Furthermore, note that the likelihood variance does not appear in the equation. If we are only interested in the maximum of the posterior we do not care about the variance. \n",
    "\n",
    "Although it is overkill we will place this operation into a class to continue our emphasis of the interaction between agent and environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearMaximumLikelihoodAgent:\n",
    "    def __init__(self, params: dict) -> None:\n",
    "        self.params = SimpleNamespace(**params)\n",
    "        \n",
    "        self.posterior_mode = None\n",
    "        \n",
    "    def infer_state(self, y: float):\n",
    "        self.posterior_mode = (np.mean(y) - self.params.beta_0) / self.params.beta_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate an observation from the environment (where $\\beta_0^*=3$ and $\\beta_1^*=2$) and infer the state with the agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment parameters\n",
    "env_params = {\n",
    "    \"beta_0_star\" : 3,    # Linear parameter intercept\n",
    "    \"beta_1_star\" : 2,    # Linear parameter slope\n",
    "    \"y_star_std\"  : 1e-5   # Standard deviation of sensory data\n",
    "}\n",
    "\n",
    "# Initialize environment\n",
    "env = create_environment(name=\"static_linear\", params=env_params)\n",
    "\n",
    "# Generate data\n",
    "x_range = np.linspace(start=0.01, stop=5, num=500)\n",
    "x_star = 2\n",
    "\n",
    "env.build(x_star)\n",
    "y = env.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The posterior mode is 1.9999948781850838. This is the most likely (expected) food size, the highest probability hidden state estimate, when the observed light intensity is 7.\n"
     ]
    }
   ],
   "source": [
    "# Agent parameters\n",
    "agent_params = {\n",
    "    \"beta_0\" : 3,    # Linear parameter intercept\n",
    "    \"beta_1\" : 2,    # Linear parameter slope\n",
    "}\n",
    "\n",
    "agent = LinearMaximumLikelihoodAgent(params=agent_params)\n",
    "agent.infer_state(y)\n",
    "posterior_mode = agent.posterior_mode\n",
    "\n",
    "print(f'The posterior mode is {posterior_mode}. This is the most likely (expected) food size, the highest probability hidden state estimate, when the observed light intensity is 7.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the hidden state matches with the true external state, $x^*=2$. Let's now use $N=30$ samples. We should not need to make any changes to our maximum likelihood agent class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment parameters\n",
    "env_params = {\n",
    "    \"beta_0_star\" : 3,    # Linear parameter intercept\n",
    "    \"beta_1_star\" : 2,    # Linear parameter slope\n",
    "    \"y_star_std\"  : 0.25   # Standard deviation of sensory data\n",
    "}\n",
    "\n",
    "# Initialize environment and agent\n",
    "env = create_environment(name=\"static_linear\", params=env_params)\n",
    "\n",
    "# Generate data for three different x_star values\n",
    "x_star  = 2                                          # 3 different external states\n",
    "N       = 30                                         # Number of samples\n",
    "y       = np.zeros(N)                                # Empty array for i=20 samples\n",
    "\n",
    "# Generate\n",
    "for i in range(N):\n",
    "    env.build(x_star)\n",
    "    y[i] = env.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The posterior mode is 1.967439738223851. This is the most likely (expected) food size, the highest probability hidden state estimate, when the observed light intensity is 7 with N=30 samples.\n"
     ]
    }
   ],
   "source": [
    "agent = LinearMaximumLikelihoodAgent(params=agent_params)\n",
    "agent.infer_state(y)\n",
    "posterior_mode = agent.posterior_mode\n",
    "\n",
    "print(f'The posterior mode is {posterior_mode}. This is the most likely (expected) food size, the highest probability hidden state estimate, when the observed light intensity is 7 with N={N} samples.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum a posteriori estimate of the hidden state\n",
    "\n",
    "The maximum a posteriori (MAP) estimate is similar to the MAP estimate except that instead of just the likelihood we also add in a prior. In other words, we have the following objective:\n",
    "\n",
    "$$\n",
    "    x^{MAP} \\triangleq \\underset{x}{\\text{argmin}} \\hspace{2mm} -\\sum^n_{i=0} \\log p(y^{(i)} \\mid x) - \\log p(x)  = -\\ell(x).  \n",
    "$$\n",
    "\n",
    "where, as before, we have taken the negative log of the expression. Following the same procedure for obtaining $x$ in the MLE case above, we can obtain a solution for the MAP case. This is the posterior mode of $x$ given all data samples under any prior beliefs the agent may have about $x$. The resulting analytic solution is:\n",
    "\n",
    "$$\n",
    "    x^{MAP} = \\frac{\\beta_1(\\bar{y} - \\beta_0) + m_x}{\\beta_1^2 + 1}\n",
    "$$\n",
    "\n",
    "Other than the linear model parameters and the data, we also need to take into account the prior mean. Below we represent this equation in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearMaximumAprioriAgent:\n",
    "    def __init__(self, params: dict) -> None:\n",
    "        self.params = SimpleNamespace(**params)\n",
    "        \n",
    "        self.posterior_mode = None\n",
    "        \n",
    "    def infer_state(self, y: float):\n",
    "        self.posterior_mode = (self.params.beta_1 * (np.mean(y) - self.params.beta_0) + self.params.m_x) / (self.params.beta_1**2 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment parameters\n",
    "env_params = {\n",
    "    \"beta_0_star\" : 3,    # Linear parameter intercept\n",
    "    \"beta_1_star\" : 2,    # Linear parameter slope\n",
    "    \"y_star_std\"  : 1e-5   # Standard deviation of sensory data\n",
    "}\n",
    "\n",
    "# Initialize environment\n",
    "env = create_environment(name=\"static_linear\", params=env_params)\n",
    "\n",
    "# Generate data\n",
    "x_range = np.linspace(start=0.01, stop=5, num=500)\n",
    "x_star = 2\n",
    "\n",
    "env.build(x_star)\n",
    "y = env.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The posterior mode is 2.4000024293255326. This is the most likely (expected) food size, the highest probability hidden state estimate, when the observed light intensity is 7.\n"
     ]
    }
   ],
   "source": [
    "# Agent parameters\n",
    "agent_params = {\n",
    "    \"beta_0\" : 3,    # Linear parameter intercept\n",
    "    \"beta_1\" : 2,    # Linear parameter slope\n",
    "    \"m_x\"    : 4,    # Prior mean\n",
    "}\n",
    "\n",
    "agent = LinearMaximumAprioriAgent(params=agent_params)\n",
    "agent.infer_state(y)\n",
    "posterior_mode = agent.posterior_mode\n",
    "\n",
    "print(f'The posterior mode is {posterior_mode}. This is the most likely (expected) food size, the highest probability hidden state estimate, when the observed light intensity is 7.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is identical to the posterior mode from the linear probabilistic generating functions notebook which utilized Bayesian inference to come to the same solution. In this case, the prior mean at $m_x = 4$ biases the model away from the data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
